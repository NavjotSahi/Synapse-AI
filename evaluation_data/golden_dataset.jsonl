{"question": "What is the role of the lexical analyzer in a compiler?", "answer": "The lexical analyzer in a compiler is responsible for recognizing tokens, discarding white spaces and comments, reporting errors, and generating a token stream. It uses regular expressions and finite state automata to identify lexemes corresponding to different token classes such as identifiers, keywords, numbers, etc."}
{"question": "How does a compiler perform semantic analysis?", "answer": "Semantic analysis in a compiler involves understanding the meaning of the parsed code by enforcing language-specific rules. It includes checks like variable binding, type checking, and resolving ambiguities that cannot be handled by syntax analysis alone."}
{"question": "What is a 'symbol table' and why is it used in compilers?", "answer": "A symbol table is a data structure used by compilers to store information about program variables, such as their names, types, memory locations, and scope. It acts as a central repository accessed by different phases of the compiler."}
{"question": "List three deep learning models used in healthcare for diagnosis.", "answer": "Three deep learning models used in healthcare for diagnosis are: Artificial Neural Networks (ANNs), Support Vector Machines (SVMs), and Decision Trees (DTs)."}
{"question": "Describe how anomaly detection is used in the banking sector.", "answer": "In the banking sector, anomaly detection is used to classify data into normal and outlier groups to expose suspicious transactions. It helps identify potentially fraudulent activities by recognizing deviations from normal behavior patterns using machine learning algorithms."}
{"question": "What is algorithmic trading and how is AI used in it?", "answer": "Algorithmic trading refers to the use of computer algorithms to automate the buying and selling of financial securities. AI is used in algorithmic trading to develop models that analyze market data, predict trends, and execute trades at high frequency and volume, improving efficiency and profitability."}
{"question": "Explain the 'maximal munch' principle in lexical analysis.", "answer": "The 'maximal munch' principle refers to the strategy of recognizing the longest possible string that matches a token pattern during lexical analysis. This ensures that the lexer consumes the maximum number of characters for each token before proceeding."}
{"question": "Does the compiler use regular expressions in code optimization?", "answer": "No, regular expressions are primarily used in the lexical analysis phase for token recognition. Code optimization involves transformations like constant folding and dead code elimination, which require semantic and syntactic analysis, not regular expressions."}
{"question": "What is the difference between model-based and memory-based collaborative filtering?", "answer": "Model-based collaborative filtering builds predictive models using techniques like matrix factorization and SVD, offering better scalability and accuracy. Memory-based collaborative filtering makes predictions based on user or item similarities using metrics like cosine similarity, but suffers from sparsity and scalability issues."}
{"question": "Can a lexical analyzer alone resolve C++ template parsing issues?", "answer": "No, lexical analyzers alone cannot resolve C++ template parsing issues due to the ambiguity in syntax such as 'Foo<Bar<Bazz>>'. These require syntactic and contextual understanding which is beyond the scope of lexical analysis."}
{"question": "What are the advantages of using deep learning in cybersecurity for banks?", "answer": "Deep learning in cybersecurity offers advantages such as automated malware detection, phishing identification, and spam filtering. Models like CNNs, RNNs, and Restricted Boltzmann Machines improve accuracy and adaptability in identifying complex threat patterns."}
{"question": "Is there any mention of quantum computing in these course materials?", "answer": "No, quantum computing is not mentioned in the provided course materials."}
